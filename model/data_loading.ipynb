{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tfd\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tfd\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, load_model \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as implt\n",
    "from IPython.display import clear_output as cls"
   ],
   "id": "3662b6e611050d63",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "IMG_WIDTH = 200\n",
    "IMG_HEIGHT = 50\n",
    "IMG_SIZE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 150\n",
    "LEARNING_RATE = 1e-3\n",
    "MODEL_NAME = \"CharacterRecognition-Model\"\n",
    "TRAIN_SIZE = BATCH_SIZE * 3000\n",
    "VALID_SIZE = BATCH_SIZE * 1500\n",
    "TEST_SIZE  = BATCH_SIZE * 300\n",
    "AUTOTUNE = tfd.AUTOTUNE\n",
    "\n",
    "# Paths \n",
    "img_path = '../dataset/img'\n",
    "transcriptions = '../dataset/transcriptions.json'\n",
    "\n",
    "# SetUp random seeds for numpy and TensorFlow\n",
    "np.random.seed(2569)\n",
    "tf.random.set_seed(2569)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the json files and transform into dataframes\n",
    "\n",
    "def json_to_csv(json_path):\n",
    "    # Load the JSON file\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Convert the data to a pandas DataFrame\n",
    "    return pd.DataFrame(data)\n",
    " \n",
    "all_transcriptions = json_to_csv(transcriptions)   \n",
    "\n",
    "# Convert the JSON files to CSV\n"
   ],
   "id": "850b77a20b2ec41f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_transcriptions.head()"
   ],
   "id": "82bfe86c98bc97ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def add_file_path(df):\n",
    "    df['file_name'] = df['img'].apply(lambda x: \"../dataset/img/\" + x)\n",
    "    return df\n",
    "\n",
    "def drop_unnecessary_columns(df):\n",
    "    return df.drop(columns=['decade_id', 'nameset'])\n",
    "\n",
    "def transform_pipeline(df):\n",
    "    return add_file_path(drop_unnecessary_columns(df))\n",
    "\n",
    "all_transcriptions = transform_pipeline(all_transcriptions)\n",
    "\n",
    "def check_img_exists(df):\n",
    "    #drop imgs that do not exist\n",
    "    return df[df['file_name'].apply(lambda x: os.path.exists(x))]\n",
    "\n",
    "all_transcriptions = check_img_exists(all_transcriptions)\n",
    "\n",
    "length = len(all_transcriptions)\n",
    "train_csv = all_transcriptions[:int(length * 0.8)]\n",
    "test_csv = all_transcriptions[int(length * 0.8):]\n",
    "\n",
    "length = len(train_csv)\n",
    "valid_csv = train_csv[int(length * 0.8):]\n",
    "train_csv = train_csv[:int(length*0.8)]\n",
    "\n",
    "print(len(train_csv['img']))\n",
    "print(len(valid_csv['img']))\n",
    "print(len(test_csv['img']))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b49ee6d2795851e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get the train labels \n",
    "train_labels = [str(word) for word in train_csv[\"text\"].to_numpy()]\n",
    "val_labels = [str(word) for word in valid_csv[\"text\"].to_numpy()]\n",
    "test_labels = [str(word) for word in test_csv[\"text\"].to_numpy()]\n",
    "\n",
    "# extract all the unique characters\n",
    "unique_characters_train = set(char for word in train_labels for char in word)\n",
    "unique_characters_test = set(char for word in test_labels for char in word)\n",
    "unique_characters_val = set(char for word in val_labels for char in word)\n",
    "\n",
    "# get the unique characters\n",
    "unique_characters = unique_characters_train.union(unique_characters_test).union(unique_characters_val)\n",
    "\n",
    "# define the number of classes (for labels) based on the number of unique characters\n",
    "n_classes = len(unique_characters)\n",
    "\n",
    "MAX_LABEL_LENGTH = max(map(len, train_labels))"
   ],
   "id": "5cc6a77bf9c79c7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "n_classes",
   "id": "a0ea01b4fba7aef1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Number of unique characters: {n_classes}\")\n",
    "temp = ['a', 'b', '\\u00e8', '\\u00f2', '\\u00e9', '\\u2013']\n",
    "\n",
    "for char in temp:\n",
    "    if char not in unique_characters:\n",
    "        print(f\"Character {char} not found in the unique characters\")"
   ],
   "id": "c2c091a3b78a55f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MAX_LABEL_LENGTH",
   "id": "cd72e52b89b57783",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_csv.head()"
   ],
   "id": "7cd6c54d01559132",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras.src.layers import StringLookup\n",
    "\n",
    "# Char to Num\n",
    "char_to_num = StringLookup(vocabulary=list(unique_characters), mask_token=None)\n",
    "num_to_char = StringLookup(vocabulary = char_to_num.get_vocabulary(), mask_token = None, invert = True)"
   ],
   "id": "808dd9d2ba44c226",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    This function gets the image path and \n",
    "    reads the image using TensorFlow, Then the image will be decoded and \n",
    "    will be converted to float data type. next resize and transpose will be applied to it.\n",
    "    In the final step the image will be converted to a Numpy Array using tf.cast\n",
    "    \"\"\"\n",
    "    # read the image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # decode the image\n",
    "    decoded_image = tf.image.decode_jpeg(contents=image, channels=1)\n",
    "    # convert image data type to float32\n",
    "    convert_imgs = tf.image.convert_image_dtype(image=decoded_image, dtype=tf.float32)\n",
    "    # resize and transpose \n",
    "    resized_image = tf.image.resize(images=convert_imgs, size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    image = tf.transpose(resized_image, perm = [1, 0, 2])\n",
    "\n",
    "    # to numpy array (Tensor)\n",
    "    image_array = tf.cast(image, dtype=tf.float32)\n",
    "\n",
    "    return image_array"
   ],
   "id": "9fa8429b917f2db2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "875d56e49f13fe64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_images = train_csv['file_name']\n",
    "train_labels = train_csv['text']"
   ],
   "id": "a04b881bcf2aa84a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def encode_single_sample(image_path, label:str):\n",
    "    if image_path is None:\n",
    "        print(\"Found None in image_path\")\n",
    "        raise ValueError(\"Image path is None\")\n",
    "    if label is None:\n",
    "        print(\"Found None in label\")\n",
    "        raise ValueError(\"Label is None\")\n",
    "    \n",
    "    # Get the image\n",
    "    image = load_image(image_path)\n",
    "    # Convert the label into characters\n",
    "    chars = tf.strings.unicode_split(label, input_encoding='UTF-8')\n",
    "    # Convert the characters into vectors\n",
    "    vecs = char_to_num(chars)\n",
    "    \n",
    "    for vec in vecs:\n",
    "        if vec is None:\n",
    "            print(\"Found None in vec\")\n",
    "            raise ValueError(\"Found None in vec after char_to_num\")\n",
    "        if vec == char_to_num('[UNK]'):\n",
    "            print(f\"Found OOV token in vec: {vec}\")\n",
    "            image_path_string = tf.py_function(func=lambda x: x.numpy().decode('utf-8'), inp=[image_path], Tout=tf.string)\n",
    "            print(tf.print(image_path, output_stream=sys.stderr))\n",
    "            raise ValueError(f\"Found OOV token and label\")\n",
    "            # Optionally handle OOV tokens here\n",
    "\n",
    "    # Pad label\n",
    "    pad_size = MAX_LABEL_LENGTH - tf.shape(vecs)[0]\n",
    "    vecs = tf.pad(vecs, paddings = [[0, pad_size]], constant_values=n_classes+1)\n",
    "\n",
    "    return {'image':image, 'label':vecs}"
   ],
   "id": "9f06364375e7e8ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training Data\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(train_csv['file_name'].to_list()), np.array(train_csv['text'].to_list()))\n",
    ").shuffle(1000).map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# Validation data\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(valid_csv['file_name'].to_list()), np.array(valid_csv['text'].to_list()))\n",
    ").map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# Testing data.\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(test_csv['file_name'].to_list()), np.array(test_csv['text'].to_list()))\n",
    ").map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)"
   ],
   "id": "615412f38707f864",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Training Data Size   : {tf.data.Dataset.cardinality(train_ds).numpy() * BATCH_SIZE}\")\n",
    "print(f\"Validation Data Size : {tf.data.Dataset.cardinality(valid_ds).numpy() * BATCH_SIZE}\")"
   ],
   "id": "ca5e6034d3cce75d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "unknown_token = char_to_num([tf.constant(\"[UNK]\")])\n",
    "\n",
    "print(\"unknown\", unknown_token)\n",
    "\n",
    "for v in iter(test_ds):\n",
    "    \n",
    "    for c in iter(v['label']):\n",
    "        if c is None:\n",
    "            print(\"Found None in label\")\n",
    "            raise ValueError(\"Found None in label\")\n",
    "        for t in c.numpy():\n",
    "            if t == 0:\n",
    "                print(\"Found 0\")\n",
    "            # if int(t) == n_classes+1:\n",
    "            #     print(\"Found OOV token in label\")\n",
    "                # raise ValueError(\"Found OOV token in label\")"
   ],
   "id": "130ebf252ef30a2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check the data distribution \n",
    "print(f\"Training Data Size   : {tf.data.Dataset.cardinality(train_ds).numpy() * BATCH_SIZE}\")\n",
    "print(f\"Validation Data Size : {tf.data.Dataset.cardinality(valid_ds).numpy() * BATCH_SIZE}\")\n",
    "print(f\"Testing Data Size    : {tf.data.Dataset.cardinality(test_ds).numpy() * BATCH_SIZE}\")\n",
    "\n"
   ],
   "id": "c1b8973185d83c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output as cls\n",
    "\n",
    "\n",
    "def show_images(data, GRID=[4,4], FIGSIZE=(25, 8), cmap='binary_r', model=None, decode_pred=None):\n",
    "    \n",
    "    # Plotting configurations\n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "    n_rows, n_cols = GRID\n",
    "    \n",
    "    # Loading Data \n",
    "    data = next(iter(data))\n",
    "    images, labels = data['image'], data['label']\n",
    "    \n",
    "    # Iterate over the data \n",
    "    for index, (image, label) in enumerate(zip(images, labels)):\n",
    "        \n",
    "        # Label processing\n",
    "        text_label = num_to_char(label)\n",
    "        text_label = tf.strings.reduce_join(text_label).numpy().decode('UTF-8')\n",
    "        text_label = text_label.replace(\"[UNK]\", \" \").strip()\n",
    "        \n",
    "        # Create a sub plot\n",
    "        plt.subplot(n_rows, n_cols, index+1)\n",
    "        plt.imshow(tf.transpose(image, perm=[1,0,2]), cmap=cmap)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if model is not None and decode_pred is not None:\n",
    "            # Make prediction\n",
    "            pred = model.predict(tf.expand_dims(image, axis=0))\n",
    "            pred = decode_pred(pred)[0]\n",
    "            title = f\"True : {text_label}\\nPred : {pred}\"\n",
    "            plt.title(title)\n",
    "        else:\n",
    "            # add title\n",
    "            plt.title(text_label)\n",
    "\n",
    "    # Show the final plot\n",
    "    cls()\n",
    "    plt.show()"
   ],
   "id": "5762ddb487c9d49a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "show_images(data=train_ds, GRID=[4,4], FIGSIZE=(25, 8))"
   ],
   "id": "890ab0c724522ccd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "distinct_elements = test_csv['img'].nunique()\n",
    "distinct_elements"
   ],
   "id": "4f95e082d1504120",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CTCLayer(Layer):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # define the loss function \n",
    "        self.loss_function = tf.keras.backend.ctc_batch_cost\n",
    "\n",
    "      \n",
    "\n",
    "    def call(self, y_true, y_hat):\n",
    "        # Get the batch length \n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "\n",
    "        # get the input and label lengths\n",
    "        input_len = tf.cast(tf.shape(y_hat)[1], dtype='int64') * tf.ones(shape=(batch_len, 1), dtype='int64')\n",
    "        label_len = tf.cast(tf.shape(y_true)[1], dtype='int64') * tf.ones(shape=(batch_len, 1), dtype='int64')\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss_function(y_true, y_hat, input_len, label_len) \n",
    "\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        return y_hat\n",
    "# from tensorflow.keras import layers, models\n",
    "# import keras\n",
    "# model = models.Sequential([\n",
    "#     # First convolutional layer\n",
    "#     layers.Conv2D(16, (3, 3), activation='relu', input_shape=(200, 50, 1)),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     \n",
    "#     # Second convolutional layer\n",
    "#     layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     \n",
    "#     # Third convolutional layer\n",
    "#     layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "#     \n",
    "#     # Fourth convolutional layer\n",
    "#     layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "#     \n",
    "#     # Flattening the 3D output to 1D before feeding it into the dense layer\n",
    "#     layers.Flatten(),\n",
    "#     \n",
    "#     # Dense layers for classification\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dense(10, activation='softmax')  # For 10 classes\n",
    "# ])\n",
    "# \n",
    "# \n",
    "# model.compile(optimizer='adam', \n",
    "#               loss='sparse_categorical_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "# \n",
    "# BATCH_SIZE = 32\n",
    "# \n",
    "# history = model.fit(\n",
    "#     train_ds,\n",
    "#     epochs=10,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     validation_data=valid_ds)"
   ],
   "id": "71ca4c35650ae232",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Input Layer\n",
    "input_images = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 1), name=\"image\")\n",
    "\n",
    "# Labels : These are added for the training purpose.\n",
    "input_labels = Input(shape=(None, ), name=\"label\")\n",
    "\n",
    "### Convolutional layers\n",
    "# layer 1 \n",
    "conv_1 = Conv2D(64, 3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\", activation=\"relu\", name=\"conv_1\")(input_images)\n",
    "# layer 2\n",
    "conv_2 = Conv2D(32, 3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\", activation=\"relu\", name=\"conv_2\")(conv_1)\n",
    "max_pool_1 = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv_2)\n",
    "# layer 3\n",
    "conv_3 = Conv2D(64, 3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal', name=\"conv_3\")(max_pool_1)\n",
    "conv_4 = Conv2D(32, 3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal', name=\"conv_4\")(conv_3)\n",
    "max_pool_2 = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv_4)\n",
    "\n",
    "\n",
    "\n",
    "### Encoding \n",
    "reshape = Reshape(target_shape=((IMG_WIDTH//4), (IMG_HEIGHT//4)*32), name=\"reshape_layer\")(max_pool_2)\n",
    "dense_encoding = Dense(64, kernel_initializer=\"he_normal\", activation=\"relu\", name=\"enconding_dense\")(reshape)\n",
    "dense_encoding_2 = Dense(64, kernel_initializer=\"he_normal\", activation=\"relu\", name=\"enconding_dense_2\")(dense_encoding)\n",
    "dropout = Dropout(0.4)(dense_encoding_2)\n",
    "\n",
    "# Decoder\n",
    "lstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.25), name=\"bidirectional_lstm_1\")(dropout)\n",
    "lstm_2 = Bidirectional(LSTM(64, return_sequences=True, dropout=0.25), name=\"bidirectional_lstm_2\")(lstm_1)\n",
    "\n",
    "# Final Output layer\n",
    "output = Dense(len(char_to_num.get_vocabulary())+1, activation=\"softmax\", name=\"output_dense\")(lstm_2)\n",
    "\n",
    "# Add the CTC loss \n",
    "# ctc_loss_layer = CTCLayer()(input_labels, output) \n",
    "# Define the final model\n",
    "model = Model(inputs=[input_images, input_labels], outputs=[output])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3f37032685cf234",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# draw the model plot\n",
    "# tf.keras.utils.plot_model(\n",
    "#     model,\n",
    "#     to_file='model-graph.png'\n",
    "# )\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa9b78b00aa75ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4f67edb421eacccc"
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f26be3bbcf8e9dea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if there are None values in your data\n",
    "for data in train_ds:\n",
    "    if None in data:\n",
    "        print(\"None values found in train_ds\")\n",
    "        break\n",
    "\n",
    "for data in valid_ds:\n",
    "    if None in data:\n",
    "        print(\"None values found in valid_ds\")\n",
    "        break"
   ],
   "id": "d49e04453628bec6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f448e7092dd56479",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def check_none_values(dataset):\n",
    "    for batch in dataset:\n",
    "        for data in batch:\n",
    "            if data is None:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Check train_ds\n",
    "if check_none_values(train_ds):\n",
    "    print(\"None values found in train_ds\")\n",
    "\n",
    "# Check valid_ds\n",
    "if check_none_values(valid_ds):\n",
    "    print(\"None values found in valid_ds\")\n",
    "\n",
    "# If no None values are found, proceed with fitting the model\n",
    "if not check_none_values(train_ds) and not check_none_values(valid_ds):\n",
    "    history = model.fit(train_ds, validation_data=valid_ds, epochs=EPOCHS)\n",
    "else:\n",
    "    print(\"Datasets contain None values. Please fix the data before training the model.\")\n",
    "\n",
    "# def filter_none_values(dataset):\n",
    "#     filtered_dataset = []\n",
    "#     for batch in dataset:\n",
    "#         if None not in batch:\n",
    "#             filtered_dataset.append(batch)\n",
    "#     return filtered_dataset\n",
    "# \n",
    "# # Filter train_ds and valid_ds\n",
    "# train_ds = filter_none_values(train_ds)\n",
    "# valid_ds = filter_none_values(valid_ds)\n",
    "# \n",
    "# # # Proceed with fitting the model\n",
    "# history = model.fit(train_ds, validation_data=valid_ds, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n"
   ],
   "id": "c3b662133e3003bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check the data distribution \n",
    "print(f\"Training Data Size   : {tf.data.Dataset.cardinality(train_ds).numpy() * BATCH_SIZE}\")\n",
    "print(f\"Validation Data Size : {tf.data.Dataset.cardinality(valid_ds).numpy() * BATCH_SIZE}\")\n",
    "print(f\"Testing Data Size    : {tf.data.Dataset.cardinality(test_ds).numpy() * BATCH_SIZE}\")"
   ],
   "id": "37c879f12ab8564",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "85aa3558af3299e9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
